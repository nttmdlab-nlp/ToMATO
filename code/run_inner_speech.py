import os
import json
import datetime
import base64
import time
import requests
import traceback
import argparse
from collections import defaultdict
from pathlib import Path
import random

import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig

from src.nn import load_model_hf, step, multi_turn_conversation
from src.utils import load_json, save_json, remove_tom, get_formatted_conv


def ArgParser():
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("--exp_id", default="default", type=str)
    parser.add_argument("--scenario_dir", default="data/scenarios", type=str)
    parser.add_argument("--scenario_list", default=None, type=str)
    parser.add_argument("--data_dir", default="", type=str, help="")
    parser.add_argument("--output_dir", default="output", type=str, help="")
    parser.add_argument("--output_suffix", default="", type=str, help="")
    parser.add_argument("--n_turn", default=5, type=int, help="")
    parser.add_argument("--conv_mode", default="", type=str, help="")
    parser.add_argument("--log_steps", default=10, type=int, help="")
    parser.add_argument("--debug", default=False, action="store_true", help="")
    parser.add_argument("--eval_batch_size", default=8, type=int, help="")
    parser.add_argument("--model_path", default="meta-llama/Meta-Llama-3-70B-Instruct", type=str)
    parser.add_argument("--disable_inner_speech", default=False, action="store_true", help="")
    parser.add_argument("--keep_inner_speech", default=False, action="store_true", help="")
    parser.add_argument("--overwrite", default=False, action="store_true", help="")
    parser.add_argument("--load_8bit", default=False, action="store_true", help="")
    parser.add_argument("--load_4bit", default=False, action="store_true", help="")
    parser.add_argument("--do_sample", default=False, action="store_true", help="")
    parser.add_argument("--top_p", default=0.9, type=float, help="")
    parser.add_argument("--temperature", default=0.6, type=float, help="")
    parser.add_argument("--seed", default=42, type=int, help="")
    args = parser.parse_args()
    print(args)
    return args


def set_seed(seed):
    torch.manual_seed(seed)


def main():
    args = ArgParser()

    print(datetime.datetime.now(), f'Started: exp_id = {args.exp_id}')

    set_seed(args.seed)

    if torch.cuda.is_available():
        device = 'cuda'
    elif torch.backends.mps.is_available():
        device = "mps"
    else:
        device = 'cpu'

    model_name = args.model_path.split('/')[1]
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    model, tokenizer = load_model_hf(args.model_path, args.load_8bit, args.load_4bit, device=device)

    if args.scenario_list is None:
        files = list(Path(args.scenario_dir).glob('*.json'))
        ids = [f.stem for f in files]
    else:
        with open(args.scenario_list, 'r') as f:
            ids = f.readlines()
            ids = [i.strip() for i in ids]

    random.shuffle(ids)
    for scenario_id in tqdm(ids):
        output_file = output_dir / f'{scenario_id}.json'
        if output_file.exists():
            conv_config = load_json(output_file)
            if 'this_will_be_generated_by' in conv_config:
                if conv_config['this_will_be_generated_by'] != args.exp_id:
                    print(output_file.name, 'will be generated by others, so it\'s skipped.')
                    continue
            else:
                save_json({'this_will_be_generated_by': args.exp_id}, output_dir / f'{scenario_id}.json')
        else:
            save_json({'this_will_be_generated_by': args.exp_id}, output_dir / f'{scenario_id}.json')
        print(output_file.name, 'will be generated.')
        scenario = load_json(Path(args.scenario_dir) / f"{scenario_id}.json")

        conv_config = {
            "n_turn": args.n_turn,
            "agent1": args.model_path,
            "agent2": args.model_path,
            "init_inst1": scenario["init_inst1"],
            "init_inst2": scenario["init_inst2"],
            "system1": scenario["system1"],
            "system2": scenario["system2"],
            "tom_prompt1": scenario["tom_prompt1"],
            "tom_prompt2": scenario["tom_prompt2"],
            "inner_speech": not args.disable_inner_speech,
            "do_sample": args.do_sample,
            "top_p": args.top_p,
            "temperature": args.temperature,
            "keep_inner_speech": args.keep_inner_speech
        }

        conv1, conv2, messages = multi_turn_conversation(
            model,
            tokenizer,
            args.conv_mode,
            **conv_config)
        conv_config["agent1"] = conv1.__dict__
        conv_config["agent2"] = conv2.__dict__
        conv_config["agent1"]["sep_style"] = conv_config["agent1"]["sep_style"].value
        conv_config["agent2"]["sep_style"] = conv_config["agent2"]["sep_style"].value
        conv_config.update(args.__dict__)
        conv_config["messages"] = messages
        if "sotopia" in scenario:
            conv_config["sotopia"] = scenario["sotopia"]

        save_json(conv_config, output_dir / f'{scenario_id}.json')

    print('Finished!')

if __name__ == '__main__':
    main()
